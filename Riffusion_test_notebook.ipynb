{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgD7ratxDzAI"
      },
      "source": [
        "# INSTALLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV0nsCCc5kwh"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9zvaC7A5pF-"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hmartiro/riffusion-inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should create 'requirements.txt' file with libraries:\n",
        "- accelerate\n",
        "- argh\n",
        "- dacite\n",
        "- demucs\n",
        "- diffusers\n",
        "- flask\n",
        "- flask_cors\n",
        "- numpy\n",
        "- pillow>=9.1.0\n",
        "- plotly\n",
        "- pydub\n",
        "- pysoundfile\n",
        "- scipy\n",
        "- soundfile\n",
        "- sox\n",
        "- streamlit>=1.18.0\n",
        "- torch\n",
        "- torchaudio==0.13.0\n",
        "- torchvision\n",
        "- transformers\n",
        "- jax\n",
        "- jaxlib\n",
        "- gradio\n",
        "\n",
        "##### ! requirements.txt file from riffusion-inference git repository have some conflicts so it is better to use this list of libraries and create file by your own"
      ],
      "metadata": {
        "id": "g3yif5N3xiZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlmCrP805sjW"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements_colab.txt\n",
        "!pip install --upgrade pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjMxF3P4Jf1B"
      },
      "source": [
        "# MAIN CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocvDbj_XU20C"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.path.append('riffusion-inference')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "from io import BytesIO\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip, concatenate_audioclips\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip #is faster than subclip function\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXcA81bmVDIb"
      },
      "outputs": [],
      "source": [
        "# PIPELINE LOADING\n",
        "pipe = DiffusionPipeline.from_pretrained(\"riffusion/riffusion-model-v1\") #pipeline loading\n",
        "if torch.cuda.is_available():\n",
        "    pipe = pipe.to(\"cuda\") #switching device to cude if it is available\n",
        "\n",
        "params = SpectrogramParams()\n",
        "converter = SpectrogramImageConverter(params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3BfF6pyJgLnm"
      },
      "outputs": [],
      "source": [
        "# MAIN FUNCTIONS\n",
        "def predict(prompt, negative_prompt, width, output_dir = ''):\n",
        "    \"\"\"\n",
        "        Function returns pipeline output converted to audiofile\n",
        "    \"\"\"\n",
        "    spec = pipe(\n",
        "        prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=width,\n",
        "    ).images[0]\n",
        "    wav = converter.audio_from_spectrogram_image(image=spec)\n",
        "    output_file = output_dir + '/output.wav'\n",
        "    wav.export(output_file, format='wav')\n",
        "    return output_file\n",
        "\n",
        "def split_clip(videofile: str, trim_by_parts: bool, parts = 2, start_point = 0, end_point = None):\n",
        "    \"\"\"\n",
        "        Function splits video and returns files' names\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    split = videofile.split('/')\n",
        "    filename, codec = split[-1].split('.')\n",
        "    path = '/'.join(split[:-1])\n",
        "    video = VideoFileClip(videofile)\n",
        "    if trim_by_parts:\n",
        "        one_subclip_duration = video.duration / parts\n",
        "        for part in range(parts - 1):\n",
        "            clips.append(f'{path}/{filename}_{part}.{codec}')\n",
        "            ffmpeg_extract_subclip(videofile, part*one_subclip_duration, (part + 1)*one_subclip_duration, targetname=clips[-1])\n",
        "        clips.append(f'{path}/{filename}_{parts - 1}.{codec}')\n",
        "        ffmpeg_extract_subclip(videofile, (parts - 1)*one_subclip_duration, video.duration, targetname=clips[-1])\n",
        "    else:\n",
        "        time_gates = []\n",
        "        if start_point > 0:\n",
        "            time_gates.append((0, start_point))\n",
        "        time_gates.append((start_point, end_point))\n",
        "        if end_point < video.duration:\n",
        "            time_gates.append((end_point, video.duration))\n",
        "        for part, (start, end) in enumerate(time_gates):\n",
        "            clips.append(f'{path}/{filename}_{part}.{codec}')\n",
        "            ffmpeg_extract_subclip(videofile, start, end, targetname=clips[-1])\n",
        "    return clips\n",
        "\n",
        "def add_audio(videofile: str, audiofile: str, overlay_audio):\n",
        "    \"\"\"\n",
        "        Function composes audio and video and returns resulting file\n",
        "    \"\"\"\n",
        "    video = VideoFileClip(videofile)\n",
        "    audio = AudioFileClip(audiofile)\n",
        "    if audio.duration < video.duration:\n",
        "        repeats = int(np.ceil(video.duration/audio.duration))\n",
        "        audio_clips = [audio] * repeats\n",
        "        audio = concatenate_audioclips(audio_clips)\n",
        "    audio = audio.subclip(0, video.duration)\n",
        "    if overlay_audio and video.audio:\n",
        "        video = video.set_audio(CompositeAudioClip([audio, video.audio]))\n",
        "    else:\n",
        "        video.audio = audio\n",
        "    filename, codec = videofile.split('.')\n",
        "    videofile = filename + '_modified.' + codec\n",
        "    video.write_videofile(videofile)\n",
        "    return videofile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z8s0SbNjadVU"
      },
      "outputs": [],
      "source": [
        "# GUI FUNCTIONS\n",
        "def on_video_upload(video):\n",
        "    length = VideoFileClip(video).duration\n",
        "    return (length,\n",
        "            gr.Number(value=0, minimum=0, maximum=length - 1, label='From (s): ', interactive=True),\n",
        "            gr.Number(value=length, minimum=1, maximum=length, label='To (s):', interactive=True))\n",
        "\n",
        "def on_parts_number_change(parts):\n",
        "    return gr.Number(value=1, minimum=1, maximum=parts, label='Part to work with: ', interactive=True)\n",
        "\n",
        "def on_start_point_change(start_point, video_duration):\n",
        "    return gr.Number(value=video_duration, minimum=start_point + 1, maximum=video_duration, label='To (s):', interactive=True)\n",
        "\n",
        "def on_split_button_click(video, parts, clip_id):\n",
        "    try:\n",
        "        clips = split_clip(video, trim_by_parts=True, parts=parts)\n",
        "        return clips[clip_id - 1], VideoFileClip(clips[clip_id - 1]).duration, clips, clip_id, gr.Button('Generate', interactive=True)\n",
        "    except Exception:\n",
        "        return None, None, None, None, gr.Button('Generate', interactive=False)\n",
        "\n",
        "def on_trim_button_click(video, start_point, end_point):\n",
        "    clip_id = 1 if start_point == 0 else 2\n",
        "    clips = split_clip(video, trim_by_parts=False, start_point=start_point, end_point=end_point)\n",
        "    return clips[clip_id - 1], VideoFileClip(clips[clip_id - 1]).duration, clips, clip_id, gr.Button('Generate', interactive=True)\n",
        "\n",
        "def on_generate_button_click(positive_prompt, negative_prompt, duration):\n",
        "    audio_width = int(np.ceil(duration*100.2/8)*8)\n",
        "    return predict(positive_prompt, negative_prompt, audio_width), gr.Button('Compose', interactive=True)\n",
        "\n",
        "def on_compose_button_click(video, audio, overlay_audi, clips, clip_number):\n",
        "    try:\n",
        "        composed_video = add_audio(video, audio, overlay_audio)\n",
        "        clips[clip_number - 1] = composed_video\n",
        "        resulting_videos = [gr.Video(clip, visible=True, interactive=False) for clip in clips]\n",
        "        return clips, gr.DownloadButton('Download (.zip)', interactive=True), gr.Button('Compose', interactive=True)\n",
        "    except Exception:\n",
        "        return None, gr.DownloadButton('Download (.zip)', interactive=False), gr.Button('Compose', interactive=False)\n",
        "\n",
        "def on_res_clips_change(resulting_clips):\n",
        "    if not resulting_clips:\n",
        "        results = [gr.Video(label='Results', interactive=False, value=None)] + [gr.Video(visible=False) for _ in range(5)]\n",
        "    else:\n",
        "        results = [gr.Video(clip, visible=True, label=clip.split('/')[-1], interactive=False, include_audio=True) for clip in resulting_clips]\n",
        "        while len(results) < 6:\n",
        "            results.append(gr.Video(visible=False))\n",
        "    return results\n",
        "\n",
        "def on_zip_button_click(clips):\n",
        "    zip_filename = \"clips.zip\"\n",
        "    with ZipFile(zip_filename, 'w') as zipf:\n",
        "        for videofile in clips:\n",
        "            zipf.write(videofile, os.path.basename(videofile))\n",
        "    return zip_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_bg6vP6Q2pn"
      },
      "outputs": [],
      "source": [
        "# INTERFACE\n",
        "with gr.Blocks() as demo:\n",
        "    # Main user inputs:\n",
        "    with gr.Row(equal_height=True):\n",
        "        video = gr.Video(label='Upload your videofile: ')\n",
        "        video_duration = gr.Number(visible=False, value=10, minimum=0)\n",
        "        with gr.Tab('Split'):\n",
        "            parts = gr.Number(value=1, minimum=1, maximum=6, label='Parts to split: ')\n",
        "            clip_number = gr.Number(label='Part to work with: ', value=1, maximum=1)\n",
        "            split_btn = gr.Button('Split')\n",
        "        with gr.Tab('Trim'):\n",
        "            start_point = gr.Number(value=0, minimum=0, label='From (s): ')\n",
        "            end_point = gr.Number(value=1, minimum=1, label='To (s):', interactive=True)\n",
        "            trim_btn = gr.Button('Trim')\n",
        "\n",
        "        clip_duration = gr.Number(visible=False, value=10, minimum=0)\n",
        "\n",
        "        with gr.Column():\n",
        "            positive_prompt = gr.Textbox(lines=2, label='Positive prompt: ')\n",
        "            negative_prompt = gr.Textbox(lines=2, label='Negative prompt: ')\n",
        "\n",
        "            generate_btn = gr.Button('Generate', interactive=False)\n",
        "\n",
        "    clips = gr.State([])\n",
        "    clip_id = gr.State([])\n",
        "\n",
        "    # Validation widgets for the user:\n",
        "    with gr.Column():\n",
        "        with gr.Row(equal_height=True):\n",
        "            videoclip = gr.Video(label='Target clip: ', interactive=False)\n",
        "            with gr.Column():\n",
        "                generated_audio = gr.Audio(label='Generated audio: ', type='filepath', interactive=False)\n",
        "                overlay_audio = gr.Checkbox(label=\"Overlay audio\", info=\"\")\n",
        "                clear_video_btn = gr.ClearButton(value='Unset target videoclip', components=[videoclip], )\n",
        "                clear_audio_btn = gr.ClearButton(value='Unset generated audio', components=[generated_audio])\n",
        "\n",
        "        compose_btn = gr.Button('Compose', interactive=False)\n",
        "\n",
        "    progress = gr.Progress()\n",
        "    resulting_clips = gr.State([])\n",
        "\n",
        "    # Outputs:\n",
        "    # TODO: make another output without maximum=6 clips (videogallery doesn't work)\n",
        "    with gr.Column():\n",
        "        with gr.Row():\n",
        "            res_v1, res_v2, res_v3 = gr.Video(label='Results'), gr.Video(visible=False), gr.Video(visible=False)\n",
        "        with gr.Row():\n",
        "            res_v4, res_v5, res_v6 = gr.Video(visible=False), gr.Video(visible=False), gr.Video(visible=False)\n",
        "\n",
        "    # Download and clear buttons:\n",
        "    with gr.Row():\n",
        "        zip_btn = gr.DownloadButton('Download (.zip)', interactive=False)\n",
        "        clear_all_btn = gr.ClearButton(\n",
        "            components=[video, positive_prompt, negative_prompt, videoclip, generated_audio, resulting_clips],\n",
        "            value='Clear all'\n",
        "        )\n",
        "\n",
        "    # EVENT CATCHERS\n",
        "    video.upload(\n",
        "        fn=on_video_upload,\n",
        "        inputs=video,\n",
        "        outputs=[video_duration, start_point, end_point]\n",
        "    )\n",
        "    parts.change(\n",
        "        fn=on_parts_number_change,\n",
        "        inputs=parts,\n",
        "        outputs=clip_number\n",
        "    )\n",
        "    start_point.change(\n",
        "        fn=on_start_point_change,\n",
        "        inputs=[start_point, video_duration],\n",
        "        outputs=end_point\n",
        "    )\n",
        "    split_btn.click(\n",
        "        fn=on_split_button_click,\n",
        "        inputs=[video, parts, clip_number],\n",
        "        outputs=[videoclip, clip_duration, clips, clip_id, generate_btn]\n",
        "    )\n",
        "    trim_btn.click(\n",
        "        fn=on_trim_button_click,\n",
        "        inputs=[video, start_point, end_point],\n",
        "        outputs=[videoclip, clip_duration, clips, clip_id, generate_btn]\n",
        "    )\n",
        "    generate_btn.click(\n",
        "        fn=on_generate_button_click,\n",
        "        inputs=[positive_prompt, negative_prompt, clip_duration],\n",
        "        outputs=[generated_audio, compose_btn]\n",
        "    )\n",
        "    compose_btn.click(\n",
        "        fn=lambda x: gr.Button('Composing video and audio... Please wait', interactive=False),\n",
        "        outputs=compose_btn\n",
        "    ).then(\n",
        "        fn=on_compose_button_click,\n",
        "        inputs=[videoclip, generated_audio, overlay_audio, clips, clip_id],\n",
        "        outputs=[resulting_clips, zip_btn, compose_btn]\n",
        "    )\n",
        "    resulting_clips.change(\n",
        "        fn=on_res_clips_change,\n",
        "        inputs=resulting_clips,\n",
        "        outputs=[res_v1, res_v2, res_v3, res_v4, res_v5, res_v6]\n",
        "    )\n",
        "    zip_btn.click(fn=lambda x: gr.DownloadButton('Downloading...', interactive=False), outputs=zip_btn).then(\n",
        "        fn=on_zip_button_click, inputs=resulting_clips\n",
        "    ).then(fn=lambda x: gr.DownloadButton('Download (.zip)', interactive=True), outputs=zip_btn)\n",
        "\n",
        "# Launching demo\n",
        "demo.launch(debug=False, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfgQHNhtSt6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}